{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "6xTh2_Vh6K6V",
    "outputId": "a3344f6c-5fd3-48c9-ad1f-4d655284bf1a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import seaborn as sns\n",
    "#import pandas_profiling\n",
    "from pydantic_settings import BaseSettings\n",
    "%matplotlib inline\n",
    "df = pd.read_csv('framingham_extended.csv')\n",
    "df_hd = pd.read_csv('heart_disease_fixed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "colab_type": "code",
    "id": "D9y8mfuS6Ph3",
    "outputId": "b972417c-9dbf-4b1a-e94b-112671ef6bfc"
   },
   "outputs": [],
   "source": [
    "# Filling out missing values\n",
    "df['BPMeds'].fillna(0, inplace = True)\n",
    "df['glucose'].fillna(df.glucose.mean(), inplace = True)\n",
    "df['totChol'].fillna(df.totChol.mean(), inplace = True)\n",
    "df['education'].fillna(1, inplace = True)\n",
    "df['BMI'].fillna(df.BMI.mean(), inplace = True)\n",
    "df['heartRate'].fillna(df.heartRate.mean(), inplace = True)\n",
    "df.isna().sum()\n",
    "df['fbs'] = (df['glucose'] > 120).astype(int)  # engineer 'fbs' since it's not available\n",
    "\n",
    "# Filling values for heart_disease dataset\n",
    "df_hd['trestbps'].fillna(df_hd.trestbps.mean(), inplace = True)\n",
    "df_hd['chol'].fillna(df_hd.chol.mean(), inplace = True)\n",
    "df_hd['result'] = (df_hd['target'] > 0).astype(int)\n",
    "\n",
    "# Fill in fbs column using sampling\n",
    "df_hd['fbs'] = pd.to_numeric(df_hd['fbs'], errors='coerce')\n",
    "fbs_dist = df_hd['fbs'].dropna().value_counts(normalize=True)\n",
    "p_0 = fbs_dist.get(0.0, 0)\n",
    "p_1 = fbs_dist.get(1.0, 0)\n",
    "total = p_0 + p_1\n",
    "p_0 /= total\n",
    "p_1 /= total\n",
    "num_missing = df_hd['fbs'].isna().sum()\n",
    "random_fill = np.random.choice([0, 1], size=num_missing, p=[p_0, p_1])\n",
    "df_hd.loc[df_hd['fbs'].isna(), 'fbs'] = random_fill\n",
    "\n",
    "shared_features = ['age', 'sex', 'trestbps', 'chol', 'fbs']\n",
    "\n",
    "df.rename(columns={\n",
    "    'male': 'sex',\n",
    "    'sysBP': 'trestbps',\n",
    "    'totChol': 'chol',\n",
    "    'heartRate': 'thalach'\n",
    "}, inplace=True)\n",
    "\n",
    "df['result'] = df['TenYearCHD'].astype(int)\n",
    "\n",
    "df_combined_fram = pd.concat([df[shared_features+['result']], df_hd[shared_features+['result']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "Ee9f0LP66RTz",
    "outputId": "1df190e3-b102-48d0-ab3d-53fde198ada2"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# separate independent & dependent variables\n",
    "X = df.loc[:,shared_features]  #independent columns\n",
    "y = df.iloc[:,-1]    #target column i.e CHD\n",
    "\n",
    "X_hd = df_hd.loc[:,shared_features]\n",
    "y_hd = df_hd.loc[:,['result']].iloc[:,0]\n",
    "\n",
    "X_concat_fram = df_combined_fram.loc[:,shared_features]\n",
    "y_concat_fram = df_combined_fram.loc[:,['result']].iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "XoDRWhfn6jQ4",
    "outputId": "f2761a2f-6979-46e4-ef30-39117c21c7ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4603, 5) (4603,)\n",
      "(1151, 5) (1151,)\n",
      "(736, 5) (736,)\n",
      "(184, 5) (184,)\n",
      "(5339, 5) (5339,)\n",
      "(1335, 5) (1335,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print (X_train.shape, y_train.shape)\n",
    "print (X_test.shape, y_test.shape)\n",
    "\n",
    "X_hd_train, X_hd_test, y_hd_train, y_hd_test = train_test_split(X_hd, y_hd, test_size=0.2)\n",
    "\n",
    "X_concat_fram_train, X_concat_fram_test, y_concat_fram_train, y_concat_fram_test = train_test_split(X_concat_fram, y_concat_fram, test_size=0.2)\n",
    "\n",
    "print (X_hd_train.shape, y_hd_train.shape)\n",
    "print (X_hd_test.shape, y_hd_test.shape)\n",
    "print (X_concat_fram_train.shape, y_concat_fram_train.shape)\n",
    "print (X_concat_fram_test.shape, y_concat_fram_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE228 optimization: MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 1. Configurable MLP Model\n",
    "class ConfigurableMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_hidden_layers=2,\n",
    "                 activation='relu', dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        # Choose activation function\n",
    "        if activation == 'relu':\n",
    "            activation_fn = nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            activation_fn = nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            activation_fn = nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(activation_fn)\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_hidden_layers):\n",
    "            if dropout_rate > 0:\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(activation_fn)\n",
    "\n",
    "        # Output layer (for binary classification â€” change if needed)\n",
    "        layers.append(nn.Linear(hidden_dim, 1))  # output logits\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 2. train model\n",
    "def train_model(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    input_dim,\n",
    "    hidden_dim=64,\n",
    "    num_hidden_layers=2,\n",
    "    activation='relu',\n",
    "    dropout_rate=0.0,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    use_l1=False,\n",
    "    l1_lambda=1e-5,\n",
    "    use_l2=False,\n",
    "    l2_lambda=1e-4,\n",
    "    early_stopping_patience=10,\n",
    "    skip_training=False\n",
    "):\n",
    "    # Build model\n",
    "    model = ConfigurableMLP(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_hidden_layers=num_hidden_layers,\n",
    "        activation=activation,\n",
    "        dropout_rate=dropout_rate if dropout_rate > 0 else 0.0\n",
    "    )\n",
    "\n",
    "    # Set optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                           weight_decay=l2_lambda if use_l2 else 0.0)\n",
    "\n",
    "    # Use BCEWithLogitsLoss for binary classification with logits\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # DataLoader setup\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = early_stopping_patience\n",
    "    best_model_state = None\n",
    "\n",
    "    # skip training for baseline results\n",
    "    if skip_training:\n",
    "        print(\"[Baseline] Skipping training, evaluating untrained model...\")\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                output = model(xb)\n",
    "                val_loss += criterion(output, yb.view(-1, 1)).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"[Baseline] Untrained Model Val Loss: {val_loss:.4f}\")\n",
    "        return model\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(xb)\n",
    "            loss = criterion(output, yb.view(-1, 1))\n",
    "\n",
    "\n",
    "            if use_l1:\n",
    "                l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "                loss += l1_lambda * l1_norm\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                output = model(xb)\n",
    "                val_loss += criterion(output, yb.view(-1,1)).item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Train Loss = {total_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = early_stopping_patience\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter -= 1\n",
    "            if patience_counter == 0:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Framingham dataset\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Fit only on Framingham\n",
    "\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Heart Disease dataset â€” use the SAME SCALER, only transform\n",
    "X_hd_scaled = scaler.transform(X_hd)\n",
    "X_hd_tensor = torch.tensor(X_hd_scaled, dtype=torch.float32)\n",
    "y_hd_tensor = torch.tensor(y_hd, dtype=torch.float32)\n",
    "\n",
    "# Optional split\n",
    "X_hd_train, X_hd_val, y_hd_train, y_hd_val = train_test_split(X_hd_tensor, y_hd_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Concatenated dataset â€” use the SAME SCALER, only transform\n",
    "X_concat_scaled = scaler.transform(X_concat_fram)\n",
    "X_concat_tensor = torch.tensor(X_concat_scaled, dtype=torch.float32)\n",
    "y_concat_tensor = torch.tensor(y_concat_fram.values, dtype=torch.float32)\n",
    "\n",
    "# Optional split\n",
    "X_concat_train, X_concat_val, y_concat_train, y_concat_val = train_test_split(X_concat_tensor, y_concat_tensor, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = torch.sigmoid(model(X)).squeeze()\n",
    "        preds = (outputs >= 0.5).int()\n",
    "        accuracy = (preds == y.int()).float().mean().item()\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline] Skipping training, evaluating untrained model...\n",
      "[Baseline] Untrained Model Val Loss: 0.6932\n"
     ]
    }
   ],
   "source": [
    "untrained_model = train_model(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    input_dim=X_train.shape[1],\n",
    "    hidden_dim=128,\n",
    "    num_hidden_layers=5,\n",
    "    activation='relu',\n",
    "    dropout_rate=0.2,\n",
    "    learning_rate=1e-3,\n",
    "    use_l1=False,\n",
    "    use_l2=True,\n",
    "    l1_lambda=1e-5,\n",
    "    l2_lambda=1e-4,\n",
    "    early_stopping_patience=100,\n",
    "    skip_training=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 62.50%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_model(untrained_model, X_hd_val, y_hd_val)\n",
    "print(f\"Baseline Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: Train Loss = 90.3212, Val Loss = 0.5966\n",
      "Epoch 2/100: Train Loss = 86.2807, Val Loss = 0.5972\n",
      "Epoch 3/100: Train Loss = 86.0327, Val Loss = 0.6005\n",
      "Epoch 4/100: Train Loss = 85.6454, Val Loss = 0.5990\n",
      "Epoch 5/100: Train Loss = 84.9117, Val Loss = 0.6015\n",
      "Epoch 6/100: Train Loss = 85.1311, Val Loss = 0.6008\n",
      "Epoch 7/100: Train Loss = 84.8212, Val Loss = 0.5966\n",
      "Epoch 8/100: Train Loss = 83.9277, Val Loss = 0.5968\n",
      "Epoch 9/100: Train Loss = 84.3032, Val Loss = 0.5990\n",
      "Epoch 10/100: Train Loss = 84.3253, Val Loss = 0.5930\n",
      "Epoch 11/100: Train Loss = 83.7851, Val Loss = 0.5957\n",
      "Epoch 12/100: Train Loss = 83.4876, Val Loss = 0.5915\n",
      "Epoch 13/100: Train Loss = 83.4653, Val Loss = 0.5950\n",
      "Epoch 14/100: Train Loss = 83.6883, Val Loss = 0.5902\n",
      "Epoch 15/100: Train Loss = 83.1642, Val Loss = 0.5950\n",
      "Epoch 16/100: Train Loss = 82.9680, Val Loss = 0.5912\n",
      "Epoch 17/100: Train Loss = 82.2477, Val Loss = 0.5861\n",
      "Epoch 18/100: Train Loss = 82.5198, Val Loss = 0.5897\n",
      "Epoch 19/100: Train Loss = 82.6290, Val Loss = 0.5958\n",
      "Epoch 20/100: Train Loss = 82.4306, Val Loss = 0.5879\n",
      "Epoch 21/100: Train Loss = 82.0383, Val Loss = 0.5872\n",
      "Epoch 22/100: Train Loss = 82.3020, Val Loss = 0.5883\n",
      "Epoch 23/100: Train Loss = 82.0465, Val Loss = 0.5884\n",
      "Epoch 24/100: Train Loss = 82.2594, Val Loss = 0.5901\n",
      "Epoch 25/100: Train Loss = 81.9879, Val Loss = 0.5891\n",
      "Epoch 26/100: Train Loss = 81.7828, Val Loss = 0.5861\n",
      "Epoch 27/100: Train Loss = 81.1084, Val Loss = 0.5857\n",
      "Epoch 28/100: Train Loss = 81.4816, Val Loss = 0.5845\n",
      "Epoch 29/100: Train Loss = 80.9680, Val Loss = 0.5931\n",
      "Epoch 30/100: Train Loss = 81.2203, Val Loss = 0.5901\n",
      "Epoch 31/100: Train Loss = 81.0835, Val Loss = 0.5861\n",
      "Epoch 32/100: Train Loss = 80.8595, Val Loss = 0.5922\n",
      "Epoch 33/100: Train Loss = 80.8221, Val Loss = 0.5849\n",
      "Epoch 34/100: Train Loss = 81.2796, Val Loss = 0.5863\n",
      "Epoch 35/100: Train Loss = 80.8565, Val Loss = 0.5804\n",
      "Epoch 36/100: Train Loss = 80.5674, Val Loss = 0.5814\n",
      "Epoch 37/100: Train Loss = 80.8876, Val Loss = 0.5790\n",
      "Epoch 38/100: Train Loss = 81.0187, Val Loss = 0.5832\n",
      "Epoch 39/100: Train Loss = 80.4977, Val Loss = 0.5823\n",
      "Epoch 40/100: Train Loss = 80.2168, Val Loss = 0.5911\n",
      "Epoch 41/100: Train Loss = 80.1169, Val Loss = 0.5868\n",
      "Epoch 42/100: Train Loss = 80.6988, Val Loss = 0.5813\n",
      "Epoch 43/100: Train Loss = 79.7294, Val Loss = 0.5834\n",
      "Epoch 44/100: Train Loss = 80.5277, Val Loss = 0.5867\n",
      "Epoch 45/100: Train Loss = 80.1715, Val Loss = 0.5871\n",
      "Epoch 46/100: Train Loss = 80.4347, Val Loss = 0.5804\n",
      "Epoch 47/100: Train Loss = 80.1620, Val Loss = 0.5869\n",
      "Epoch 48/100: Train Loss = 80.2530, Val Loss = 0.5809\n",
      "Epoch 49/100: Train Loss = 79.7339, Val Loss = 0.5825\n",
      "Epoch 50/100: Train Loss = 79.9077, Val Loss = 0.5832\n",
      "Epoch 51/100: Train Loss = 79.7507, Val Loss = 0.5748\n",
      "Epoch 52/100: Train Loss = 79.4045, Val Loss = 0.5796\n",
      "Epoch 53/100: Train Loss = 80.0299, Val Loss = 0.5759\n",
      "Epoch 54/100: Train Loss = 79.5582, Val Loss = 0.5786\n",
      "Epoch 55/100: Train Loss = 79.7844, Val Loss = 0.5878\n",
      "Epoch 56/100: Train Loss = 79.7510, Val Loss = 0.5884\n",
      "Epoch 57/100: Train Loss = 78.9580, Val Loss = 0.5857\n",
      "Epoch 58/100: Train Loss = 78.9379, Val Loss = 0.5857\n",
      "Epoch 59/100: Train Loss = 78.9734, Val Loss = 0.5824\n",
      "Epoch 60/100: Train Loss = 79.5374, Val Loss = 0.5779\n",
      "Epoch 61/100: Train Loss = 79.3991, Val Loss = 0.5802\n",
      "Epoch 62/100: Train Loss = 79.2184, Val Loss = 0.5763\n",
      "Epoch 63/100: Train Loss = 78.6807, Val Loss = 0.5779\n",
      "Epoch 64/100: Train Loss = 79.0260, Val Loss = 0.5798\n",
      "Epoch 65/100: Train Loss = 78.7336, Val Loss = 0.5860\n",
      "Epoch 66/100: Train Loss = 79.5692, Val Loss = 0.5722\n",
      "Epoch 67/100: Train Loss = 79.4578, Val Loss = 0.5770\n",
      "Epoch 68/100: Train Loss = 79.0835, Val Loss = 0.5777\n",
      "Epoch 69/100: Train Loss = 79.0496, Val Loss = 0.5804\n",
      "Epoch 70/100: Train Loss = 79.0143, Val Loss = 0.5776\n",
      "Epoch 71/100: Train Loss = 78.6647, Val Loss = 0.5799\n",
      "Epoch 72/100: Train Loss = 79.2995, Val Loss = 0.5814\n",
      "Epoch 73/100: Train Loss = 78.4280, Val Loss = 0.5792\n",
      "Epoch 74/100: Train Loss = 78.7546, Val Loss = 0.5715\n",
      "Epoch 75/100: Train Loss = 78.0206, Val Loss = 0.5812\n",
      "Epoch 76/100: Train Loss = 78.3615, Val Loss = 0.5751\n",
      "Epoch 77/100: Train Loss = 78.0920, Val Loss = 0.5817\n",
      "Epoch 78/100: Train Loss = 79.2383, Val Loss = 0.5771\n",
      "Epoch 79/100: Train Loss = 78.3074, Val Loss = 0.5786\n",
      "Epoch 80/100: Train Loss = 78.6317, Val Loss = 0.5781\n",
      "Epoch 81/100: Train Loss = 78.6485, Val Loss = 0.5804\n",
      "Epoch 82/100: Train Loss = 79.0822, Val Loss = 0.5793\n",
      "Epoch 83/100: Train Loss = 78.3912, Val Loss = 0.5721\n",
      "Epoch 84/100: Train Loss = 78.6567, Val Loss = 0.5766\n",
      "Epoch 85/100: Train Loss = 78.1648, Val Loss = 0.5763\n",
      "Epoch 86/100: Train Loss = 77.8389, Val Loss = 0.5864\n",
      "Epoch 87/100: Train Loss = 77.9842, Val Loss = 0.5822\n",
      "Epoch 88/100: Train Loss = 78.0625, Val Loss = 0.5713\n",
      "Epoch 89/100: Train Loss = 77.7204, Val Loss = 0.5827\n",
      "Epoch 90/100: Train Loss = 78.5701, Val Loss = 0.5824\n",
      "Epoch 91/100: Train Loss = 77.6825, Val Loss = 0.5743\n",
      "Epoch 92/100: Train Loss = 78.1656, Val Loss = 0.5774\n",
      "Epoch 93/100: Train Loss = 77.7982, Val Loss = 0.5844\n",
      "Epoch 94/100: Train Loss = 78.5725, Val Loss = 0.5716\n",
      "Epoch 95/100: Train Loss = 78.2299, Val Loss = 0.5767\n",
      "Epoch 96/100: Train Loss = 77.2976, Val Loss = 0.5774\n",
      "Epoch 97/100: Train Loss = 77.7797, Val Loss = 0.5742\n",
      "Epoch 98/100: Train Loss = 78.0199, Val Loss = 0.5761\n",
      "Epoch 99/100: Train Loss = 77.1826, Val Loss = 0.5781\n",
      "Epoch 100/100: Train Loss = 77.6646, Val Loss = 0.5790\n"
     ]
    }
   ],
   "source": [
    "fram_trained_model = train_model(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    input_dim=X_train.shape[1],\n",
    "    hidden_dim=64,\n",
    "    num_hidden_layers=3,\n",
    "    activation='relu',\n",
    "    dropout_rate=0.2,\n",
    "    learning_rate=1e-3,\n",
    "    use_l1=False,\n",
    "    use_l2=True,\n",
    "    l1_lambda=1e-5,\n",
    "    l2_lambda=1e-4,\n",
    "    early_stopping_patience=100,\n",
    "    skip_training=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Framingham Dataset Trained Accuracy: 62.50%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_model(fram_trained_model, X_hd_val, y_hd_val)\n",
    "print(f\"Framingham Dataset Trained Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: Train Loss = 19.1004, Val Loss = 0.6327\n",
      "Epoch 2/100: Train Loss = 16.0704, Val Loss = 0.6509\n",
      "Epoch 3/100: Train Loss = 15.9951, Val Loss = 0.6382\n",
      "Epoch 4/100: Train Loss = 15.7964, Val Loss = 0.6434\n",
      "Epoch 5/100: Train Loss = 15.5925, Val Loss = 0.6444\n",
      "Epoch 6/100: Train Loss = 15.6141, Val Loss = 0.6851\n",
      "Epoch 7/100: Train Loss = 15.6357, Val Loss = 0.6347\n",
      "Epoch 8/100: Train Loss = 15.3341, Val Loss = 0.6696\n",
      "Epoch 9/100: Train Loss = 15.2322, Val Loss = 0.6375\n",
      "Epoch 10/100: Train Loss = 15.4317, Val Loss = 0.6693\n",
      "Epoch 11/100: Train Loss = 15.1548, Val Loss = 0.6496\n",
      "Epoch 12/100: Train Loss = 15.4260, Val Loss = 0.6570\n",
      "Epoch 13/100: Train Loss = 15.2905, Val Loss = 0.6397\n",
      "Epoch 14/100: Train Loss = 15.2603, Val Loss = 0.6397\n",
      "Epoch 15/100: Train Loss = 15.1736, Val Loss = 0.6412\n",
      "Epoch 16/100: Train Loss = 15.4337, Val Loss = 0.6710\n",
      "Epoch 17/100: Train Loss = 15.1890, Val Loss = 0.6537\n",
      "Epoch 18/100: Train Loss = 15.3578, Val Loss = 0.6603\n",
      "Epoch 19/100: Train Loss = 15.1481, Val Loss = 0.6395\n",
      "Epoch 20/100: Train Loss = 15.1355, Val Loss = 0.6473\n",
      "Epoch 21/100: Train Loss = 15.1502, Val Loss = 0.6479\n",
      "Epoch 22/100: Train Loss = 15.0978, Val Loss = 0.6524\n",
      "Epoch 23/100: Train Loss = 15.1019, Val Loss = 0.6342\n",
      "Epoch 24/100: Train Loss = 15.0363, Val Loss = 0.6431\n",
      "Epoch 25/100: Train Loss = 15.3294, Val Loss = 0.6453\n",
      "Epoch 26/100: Train Loss = 15.1257, Val Loss = 0.6444\n",
      "Epoch 27/100: Train Loss = 15.2022, Val Loss = 0.6443\n",
      "Epoch 28/100: Train Loss = 15.0164, Val Loss = 0.6385\n",
      "Epoch 29/100: Train Loss = 15.0849, Val Loss = 0.6431\n",
      "Epoch 30/100: Train Loss = 15.2183, Val Loss = 0.6469\n",
      "Epoch 31/100: Train Loss = 15.0089, Val Loss = 0.6366\n",
      "Epoch 32/100: Train Loss = 14.9831, Val Loss = 0.6371\n",
      "Epoch 33/100: Train Loss = 14.9522, Val Loss = 0.6327\n",
      "Epoch 34/100: Train Loss = 15.0059, Val Loss = 0.6419\n",
      "Epoch 35/100: Train Loss = 15.0582, Val Loss = 0.6368\n",
      "Epoch 36/100: Train Loss = 15.0460, Val Loss = 0.6303\n",
      "Epoch 37/100: Train Loss = 14.9261, Val Loss = 0.6468\n",
      "Epoch 38/100: Train Loss = 14.9891, Val Loss = 0.6397\n",
      "Epoch 39/100: Train Loss = 14.9659, Val Loss = 0.6369\n",
      "Epoch 40/100: Train Loss = 14.9819, Val Loss = 0.6308\n",
      "Epoch 41/100: Train Loss = 14.6374, Val Loss = 0.6350\n",
      "Epoch 42/100: Train Loss = 15.0015, Val Loss = 0.6423\n",
      "Epoch 43/100: Train Loss = 14.8455, Val Loss = 0.6348\n",
      "Epoch 44/100: Train Loss = 14.8828, Val Loss = 0.6384\n",
      "Epoch 45/100: Train Loss = 14.7981, Val Loss = 0.6354\n",
      "Epoch 46/100: Train Loss = 14.8626, Val Loss = 0.6359\n",
      "Epoch 47/100: Train Loss = 15.1095, Val Loss = 0.6469\n",
      "Epoch 48/100: Train Loss = 14.8517, Val Loss = 0.6229\n",
      "Epoch 49/100: Train Loss = 14.7391, Val Loss = 0.6425\n",
      "Epoch 50/100: Train Loss = 14.5882, Val Loss = 0.6276\n",
      "Epoch 51/100: Train Loss = 15.0289, Val Loss = 0.6331\n",
      "Epoch 52/100: Train Loss = 14.9681, Val Loss = 0.6541\n",
      "Epoch 53/100: Train Loss = 14.9257, Val Loss = 0.6280\n",
      "Epoch 54/100: Train Loss = 14.9318, Val Loss = 0.6319\n",
      "Epoch 55/100: Train Loss = 14.8491, Val Loss = 0.6266\n",
      "Epoch 56/100: Train Loss = 14.6992, Val Loss = 0.6337\n",
      "Epoch 57/100: Train Loss = 14.7893, Val Loss = 0.6404\n",
      "Epoch 58/100: Train Loss = 14.6906, Val Loss = 0.6270\n",
      "Epoch 59/100: Train Loss = 14.7452, Val Loss = 0.6369\n",
      "Epoch 60/100: Train Loss = 14.6698, Val Loss = 0.6295\n",
      "Epoch 61/100: Train Loss = 14.6021, Val Loss = 0.6228\n",
      "Epoch 62/100: Train Loss = 14.7353, Val Loss = 0.6417\n",
      "Epoch 63/100: Train Loss = 14.7649, Val Loss = 0.6192\n",
      "Epoch 64/100: Train Loss = 14.8346, Val Loss = 0.6299\n",
      "Epoch 65/100: Train Loss = 14.6197, Val Loss = 0.6303\n",
      "Epoch 66/100: Train Loss = 14.5946, Val Loss = 0.6210\n",
      "Epoch 67/100: Train Loss = 14.7721, Val Loss = 0.6269\n",
      "Epoch 68/100: Train Loss = 14.5805, Val Loss = 0.6208\n",
      "Epoch 69/100: Train Loss = 14.6967, Val Loss = 0.6299\n",
      "Epoch 70/100: Train Loss = 14.6104, Val Loss = 0.6257\n",
      "Epoch 71/100: Train Loss = 14.6891, Val Loss = 0.6207\n",
      "Epoch 72/100: Train Loss = 14.4544, Val Loss = 0.6132\n",
      "Epoch 73/100: Train Loss = 14.6181, Val Loss = 0.6413\n",
      "Epoch 74/100: Train Loss = 14.5726, Val Loss = 0.6157\n",
      "Epoch 75/100: Train Loss = 14.4377, Val Loss = 0.6222\n",
      "Epoch 76/100: Train Loss = 14.7388, Val Loss = 0.6444\n",
      "Epoch 77/100: Train Loss = 14.7239, Val Loss = 0.6186\n",
      "Epoch 78/100: Train Loss = 14.4196, Val Loss = 0.6208\n",
      "Epoch 79/100: Train Loss = 14.5383, Val Loss = 0.6226\n",
      "Epoch 80/100: Train Loss = 14.6231, Val Loss = 0.6158\n",
      "Epoch 81/100: Train Loss = 14.4710, Val Loss = 0.6322\n",
      "Epoch 82/100: Train Loss = 14.5487, Val Loss = 0.6208\n",
      "Epoch 83/100: Train Loss = 14.5326, Val Loss = 0.6126\n",
      "Epoch 84/100: Train Loss = 14.6738, Val Loss = 0.6364\n",
      "Epoch 85/100: Train Loss = 14.6320, Val Loss = 0.6243\n",
      "Epoch 86/100: Train Loss = 14.4373, Val Loss = 0.6188\n",
      "Epoch 87/100: Train Loss = 14.5458, Val Loss = 0.6250\n",
      "Epoch 88/100: Train Loss = 14.5418, Val Loss = 0.6363\n",
      "Epoch 89/100: Train Loss = 14.6368, Val Loss = 0.6329\n",
      "Epoch 90/100: Train Loss = 14.5623, Val Loss = 0.6350\n",
      "Epoch 91/100: Train Loss = 14.3671, Val Loss = 0.6350\n",
      "Epoch 92/100: Train Loss = 14.4459, Val Loss = 0.6309\n",
      "Epoch 93/100: Train Loss = 14.3944, Val Loss = 0.6119\n",
      "Epoch 94/100: Train Loss = 14.4528, Val Loss = 0.6324\n",
      "Epoch 95/100: Train Loss = 14.6866, Val Loss = 0.6355\n",
      "Epoch 96/100: Train Loss = 14.4702, Val Loss = 0.6318\n",
      "Epoch 97/100: Train Loss = 14.6308, Val Loss = 0.6220\n",
      "Epoch 98/100: Train Loss = 14.4417, Val Loss = 0.6179\n",
      "Epoch 99/100: Train Loss = 14.3680, Val Loss = 0.6260\n",
      "Epoch 100/100: Train Loss = 14.3111, Val Loss = 0.6214\n"
     ]
    }
   ],
   "source": [
    "hd_trained_model = train_model(\n",
    "    X_hd_train, y_hd_train,\n",
    "    X_hd_val, y_hd_val,\n",
    "    input_dim=X_train.shape[1],\n",
    "    hidden_dim=64,\n",
    "    num_hidden_layers=3,\n",
    "    activation='relu',\n",
    "    dropout_rate=0.2,\n",
    "    learning_rate=1e-3,\n",
    "    use_l1=False,\n",
    "    use_l2=True,\n",
    "    l1_lambda=1e-5,\n",
    "    l2_lambda=1e-4,\n",
    "    early_stopping_patience=100,\n",
    "    skip_training=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heart_Disease Dataset Trained Accuracy: 57.07%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_model(hd_trained_model, X_hd_val, y_hd_val)\n",
    "print(f\"Heart_Disease Dataset Trained Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False) tensor(False)\n",
      "tensor(0.) tensor(1.)\n",
      "tensor(-9.3399) tensor(620.0317)\n"
     ]
    }
   ],
   "source": [
    "print(torch.isnan(X_concat_train).any(), torch.isnan(y_concat_train).any())\n",
    "print(torch.min(y_concat_train), torch.max(y_concat_train))\n",
    "print(torch.min(X_concat_train), torch.max(X_concat_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: Train Loss = 113.2882, Val Loss = 0.6223\n",
      "Epoch 2/100: Train Loss = 105.0922, Val Loss = 0.6036\n",
      "Epoch 3/100: Train Loss = 104.1281, Val Loss = 0.5991\n",
      "Epoch 4/100: Train Loss = 102.8736, Val Loss = 0.5968\n",
      "Epoch 5/100: Train Loss = 102.4142, Val Loss = 0.5963\n",
      "Epoch 6/100: Train Loss = 102.3316, Val Loss = 0.5898\n",
      "Epoch 7/100: Train Loss = 101.8708, Val Loss = 0.5914\n",
      "Epoch 8/100: Train Loss = 101.2962, Val Loss = 0.5943\n",
      "Epoch 9/100: Train Loss = 101.5385, Val Loss = 0.5889\n",
      "Epoch 10/100: Train Loss = 101.1128, Val Loss = 0.5901\n",
      "Epoch 11/100: Train Loss = 101.6787, Val Loss = 0.5890\n",
      "Epoch 12/100: Train Loss = 101.4259, Val Loss = 0.5876\n",
      "Epoch 13/100: Train Loss = 100.8322, Val Loss = 0.5889\n",
      "Epoch 14/100: Train Loss = 100.7858, Val Loss = 0.5847\n",
      "Epoch 15/100: Train Loss = 100.9045, Val Loss = 0.5922\n",
      "Epoch 16/100: Train Loss = 100.5446, Val Loss = 0.5910\n",
      "Epoch 17/100: Train Loss = 100.4566, Val Loss = 0.5911\n",
      "Epoch 18/100: Train Loss = 100.0802, Val Loss = 0.5866\n",
      "Epoch 19/100: Train Loss = 100.5188, Val Loss = 0.5848\n",
      "Epoch 20/100: Train Loss = 100.1401, Val Loss = 0.5927\n",
      "Epoch 21/100: Train Loss = 99.9500, Val Loss = 0.5889\n",
      "Epoch 22/100: Train Loss = 99.9747, Val Loss = 0.5860\n",
      "Epoch 23/100: Train Loss = 99.2650, Val Loss = 0.5852\n",
      "Epoch 24/100: Train Loss = 99.7300, Val Loss = 0.5895\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "concat_model = train_model(\n",
    "    X_concat_train, y_concat_train, \n",
    "    X_concat_val, y_concat_val,\n",
    "    input_dim=X_concat_train.shape[1],\n",
    "    hidden_dim=64,\n",
    "    num_hidden_layers=3,\n",
    "    activation='relu',\n",
    "    dropout_rate=0.2,\n",
    "    learning_rate=1e-3,\n",
    "    use_l1=False,\n",
    "    use_l2=True,\n",
    "    l1_lambda=1e-5,\n",
    "    l2_lambda=1e-4,\n",
    "    early_stopping_patience=10,\n",
    "    skip_training=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining Datasets Trained Accuracy: 61.96%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_model(concat_model, X_hd_val, y_hd_val)\n",
    "print(f\"Combining Datasets Trained Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled15.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
